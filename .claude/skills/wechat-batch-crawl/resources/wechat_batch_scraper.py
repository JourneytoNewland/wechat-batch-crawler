#!/usr/bin/env python3
"""
å¾®ä¿¡å…¬ä¼—å·æ‰¹é‡çˆ¬è™«
ä½¿ç”¨ subprocess + curl é¿å…è¢«è¯†åˆ«ä¸º 502
æ”¯æŒæ™ºèƒ½å»¶è¿Ÿã€æ—¥æœŸè¿‡æ»¤ã€è‡ªåŠ¨å»é‡
"""

import subprocess
import random
import json
import time
import sys
import re
from datetime import datetime, timedelta
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Optional, List, Dict


class WeChatBatchScraper:
    """å¾®ä¿¡å…¬ä¼—å·æ‰¹é‡çˆ¬è™«"""

    def __init__(self, config_path: Optional[str] = None):
        """åˆå§‹åŒ–çˆ¬è™«é…ç½®"""
        self.config = self._load_config(config_path)
        self.rss_url = self.config['rss_feed_url']
        self.output_dir = Path(self.config['output_base_dir'])
        self.max_workers = self.config.get('max_workers', 3)
        self.delay_range = self.config.get('delay_range', [5, 15])
        self.retry_limit = self.config.get('retry_limit', 3)
        self.timeout = self.config.get('timeout', 30)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # åŠ è½½å…ƒæ•°æ®
        self.metadata = self._load_metadata()

    def _load_config(self, config_path: Optional[str]) -> Dict:
        """ä» metadata.json åŠ è½½é…ç½®"""
        if config_path is None:
            config_path = Path(__file__).parent.parent / 'metadata.json'

        with open(config_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('configuration', {})

    def fetch_rss(self) -> str:
        """ä½¿ç”¨ curl è·å– RSS Feed (é¿å… 502)"""
        cmd = [
            'curl', '-s', '-L',
            '-H', 'User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            '-H', 'Accept: application/rss+xml, application/xml, text/xml, */*',
            '-H', 'Accept-Language: zh-CN,zh;q=0.9,en;q=0.8',
            '-m', str(self.timeout),
            '--connect-timeout', '10',
            self.rss_url
        ]

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=self.timeout + 5
        )

        if result.returncode != 0:
            raise Exception(f"RSS è·å–å¤±è´¥: {result.stderr}")

        return result.stdout

    def parse_rss(self, rss_content: str, target_date: str) -> List[Dict]:
        """è§£æ RSS Feed å¹¶æŒ‰æ—¥æœŸè¿‡æ»¤"""
        try:
            import feedparser
        except ImportError:
            raise ImportError("ç¼ºå°‘ä¾èµ–: pip install feedparser")

        feed = feedparser.parse(rss_content)
        target_dt = self._parse_date(target_date)

        articles = []
        for entry in feed.entries:
            # è§£æå‘å¸ƒæ—¶é—´
            pub_time = None
            if hasattr(entry, 'published_parsed'):
                pub_time = datetime(*entry.published_parsed[:6])
            elif hasattr(entry, 'updated_parsed'):
                pub_time = datetime(*entry.updated_parsed[:6])

            # æŒ‰æ—¥æœŸè¿‡æ»¤
            if pub_time and pub_time.date() == target_dt.date():
                articles.append({
                    'title': entry.get('title', 'æ— æ ‡é¢˜'),
                    'url': entry.get('link', ''),
                    'published': pub_time.strftime('%Y-%m-%d %H:%M:%S'),
                    'author': entry.get('author', 'æœªçŸ¥')
                })

        return articles

    def _parse_date(self, date_filter: str) -> datetime:
        """è§£ææ—¥æœŸå‚æ•°"""
        today = datetime.now()

        if date_filter == 'today':
            return today
        elif date_filter == 'yesterday':
            return today - timedelta(days=1)
        else:
            # å°è¯•è§£æ YYYY-MM-DD æ ¼å¼
            try:
                return datetime.strptime(date_filter, '%Y-%m-%d')
            except ValueError:
                raise ValueError(f"ä¸æ”¯æŒçš„æ—¥æœŸæ ¼å¼: {date_filter}")

    def get_adaptive_delay(self) -> float:
        """æ™ºèƒ½å»¶è¿Ÿ:æ ¹æ®æ—¶é—´æ®µè°ƒæ•´"""
        hour = datetime.now().hour

        if 9 <= hour <= 18:    # ç™½å¤©é«˜å³°
            return random.uniform(10, 15)
        elif 19 <= hour <= 23:   # æ™šé—´
            return random.uniform(7, 12)
        else:                     # æ·±å¤œ
            return random.uniform(3, 7)

    def filter_existing(self, urls: List[str], date_str: str) -> List[str]:
        """å»é‡:è¿‡æ»¤å·²çˆ¬å–çš„æ–‡ç« """
        output_path = self.output_dir / date_str

        if not output_path.exists():
            return urls

        metadata_file = output_path / '_metadata.json'
        if metadata_file.exists():
            with open(metadata_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                existing = set(data.get('scraped_urls', []))
                return [url for url in urls if url not in existing]

        return urls

    def scrape_article(self, url: str) -> Dict:
        """çˆ¬å–å•ç¯‡æ–‡ç« """
        for retry in range(self.retry_limit):
            try:
                html = self._fetch_html_via_curl(url)
                article_data = self._extract_content(html)
                return {
                    'url': url,
                    'success': True,
                    'data': article_data
                }
            except Exception as e:
                if retry < self.retry_limit - 1:
                    time.sleep(self.get_adaptive_delay() * 2)
                    continue
                return {
                    'url': url,
                    'success': False,
                    'error': str(e)
                }

        return {'url': url, 'success': False, 'error': 'Max retries exceeded'}

    def _fetch_html_via_curl(self, url: str) -> str:
        """ä½¿ç”¨ curl è·å– HTML (é¿å… 502)"""
        cmd = [
            'curl', '-s', '-L',
            '-H', 'User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            '-H', 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9',
            '-H', 'Accept-Language: zh-CN,zh;q=0.9,en;q=0.8',
            '-H', 'Cache-Control: no-cache',
            '-m', str(self.timeout),
            '--connect-timeout', '10',
            url
        ]

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=self.timeout + 5
        )

        if result.returncode != 0:
            raise Exception(f"HTTP è¯·æ±‚å¤±è´¥: {result.stderr}")

        return result.stdout

    def _extract_content(self, html: str) -> Dict:
        """ä» HTML æå–å†…å®¹"""
        try:
            from bs4 import BeautifulSoup
        except ImportError:
            raise ImportError("ç¼ºå°‘ä¾èµ–: pip install beautifulsoup4")

        soup = BeautifulSoup(html, 'html.parser')

        # æå–æ ‡é¢˜
        title = ''
        title_meta = soup.find('meta', property='og:title')
        if title_meta:
            title = title_meta.get('content', '')
        if not title:
            title_tag = soup.find('title')
            if title_tag:
                title = title_tag.get_text()

        # æå–ä½œè€…
        author = ''
        author_meta = soup.find('meta', property='og:article:author')
        if author_meta:
            author = author_meta.get('content', '')

        # æå–æ­£æ–‡
        content_div = soup.find('div', {'id': 'js_content'}) or soup.find('div', class_='rich_media_content')
        if content_div:
            try:
                import html2text
                h = html2text.HTML2Text()
                h.ignore_links = False
                h.ignore_images = False
                content = h.handle(str(content_div))
            except ImportError:
                raise ImportError("ç¼ºå°‘ä¾èµ–: pip install html2text")
        else:
            content = '[æ— æ³•æå–æ­£æ–‡]'

        # æå–å‘å¸ƒæ—¶é—´
        publish_time = ''
        time_meta = soup.find('meta', property='og:article:published_time')
        if time_meta:
            publish_time = time_meta.get('content', '')

        return {
            'title': title,
            'author': author,
            'content': content,
            'publish_time': publish_time
        }

    def save_article(self, article: Dict, date_str: str, index: int) -> Path:
        """ä¿å­˜æ–‡ç« ä¸º Markdown æ–‡ä»¶"""
        # åˆ›å»ºæ—¥æœŸç›®å½•
        date_dir = self.output_dir / date_str
        date_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆæ–‡ä»¶å (ç§»é™¤éæ³•å­—ç¬¦)
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', article['data']['title'][:50])
        filename = f"{index:03d}_{safe_title}.md"
        file_path = date_dir / filename

        # ç”Ÿæˆ Markdown å†…å®¹
        md_content = f"""---
title: {article['data']['title']}
author: {article['data']['author']}
publish_time: {article['data']['publish_time']}
url: {article['url']}
crawl_time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
---

# {article['data']['title']}

**ä½œè€…**: {article['data']['author']}
**å‘å¸ƒæ—¶é—´**: {article['data']['publish_time']}
**åŸæ–‡é“¾æ¥**: {article['url']}

---

{article['data']['content']}
"""

        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(md_content)

        return file_path

    def _load_metadata(self) -> Dict:
        """åŠ è½½å…ƒæ•°æ®"""
        metadata_file = self.output_dir / '_metadata.json'
        if metadata_file.exists():
            with open(metadata_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {
            'version': '1.0',
            'crawled_urls': {},
            'statistics': {'total_crawled': 0, 'success_count': 0, 'failed_count': 0}
        }

    def _update_metadata(self, date_str: str, results: List[Dict]):
        """æ›´æ–°å…ƒæ•°æ®"""
        date_dir = self.output_dir / date_str
        metadata_file = date_dir / '_metadata.json'

        # åˆå§‹åŒ–æ—¥æœŸå…ƒæ•°æ®
        date_metadata = {
            'date': date_str,
            'total_articles': len(results),
            'successful_scrapes': sum(1 for r in results if r['success']),
            'failed_scrapes': sum(1 for r in results if not r['success']),
            'scraped_urls': []
        }

        # è®°å½•çˆ¬å–ç»“æœ
        scraped_urls = []
        for result in results:
            if result['success']:
                scraped_urls.append(result['url'])

        date_metadata['scraped_urls'] = scraped_urls

        # ä¿å­˜æ—¥æœŸå…ƒæ•°æ®
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(date_metadata, f, ensure_ascii=False, indent=2)

        # æ›´æ–°å…¨å±€å…ƒæ•°æ®
        for result in results:
            url = result['url']
            if url not in self.metadata['crawled_urls']:
                self.metadata['crawled_urls'][url] = {
                    'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'status': 'success' if result['success'] else 'failed'
                }
                if not result['success']:
                    self.metadata['crawled_urls'][url]['error'] = result.get('error', 'Unknown')

        self.metadata['statistics']['total_crawled'] += len(results)
        self.metadata['statistics']['success_count'] += sum(1 for r in results if r['success'])
        self.metadata['statistics']['failed_count'] += sum(1 for r in results if not r['success'])

        # ä¿å­˜å…¨å±€å…ƒæ•°æ®
        global_metadata_file = self.output_dir / '_metadata.json'
        with open(global_metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, ensure_ascii=False, indent=2)

    def _generate_report(self, results: List[Dict], date_str: str) -> Dict:
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        total = len(results)
        success = sum(1 for r in results if r['success'])
        failed = total - success

        return {
            'date': date_str,
            'total': total,
            'success': success,
            'failed': failed,
            'success_rate': f"{success/total*100:.1f}%" if total > 0 else "0%",
            'errors': [r for r in results if not r['success']]
        }

    def run(self, target_date: str, list_only: bool = False) -> Dict:
        """ä¸»æ‰§è¡Œæµç¨‹"""
        print(f"ğŸš€ å¼€å§‹çˆ¬å– {target_date} çš„æ–‡ç« ...")

        # 1. è·å– RSS
        try:
            rss_content = self.fetch_rss()
            print(f"âœ… RSS è·å–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ RSS è·å–å¤±è´¥: {e}")
            return {'error': str(e)}

        # 2. è§£æå¹¶è¿‡æ»¤æ—¥æœŸ
        articles = self.parse_rss(rss_content, target_date)
        print(f"ğŸ“… æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")

        if not articles:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ç« ")
            return {'total': 0, 'success': 0, 'failed': 0}

        # 3. å»é‡
        urls = [a['url'] for a in articles]
        filtered_urls = self.filter_existing(urls, target_date)
        print(f"ğŸ”„ å»é‡åå¾…çˆ¬å–: {len(filtered_urls)} ç¯‡")

        if list_only:
            return {
                'articles': articles,
                'count': len(articles),
                'new_count': len(filtered_urls)
            }

        if not filtered_urls:
            print("âœ… æ‰€æœ‰æ–‡ç« å·²çˆ¬å–,æ— éœ€é‡å¤")
            return {'total': 0, 'success': 0, 'failed': 0, 'skipped': len(articles)}

        # 4. å¤šçº¿ç¨‹çˆ¬å–
        results = []
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {executor.submit(self.scrape_article, url): url for url in filtered_urls}

            for i, future in enumerate(as_completed(futures), 1):
                try:
                    result = future.result()
                    results.append(result)

                    if result['success']:
                        # ä¿å­˜æ–‡ç« 
                        self.save_article(result, target_date, i)
                        print(f"âœ… [{i}/{len(filtered_urls)}] {result['data']['title'][:40]}")
                    else:
                        print(f"âŒ [{i}/{len(filtered_urls)}] {result['url'][:60]} - {result.get('error', 'Unknown')}")

                    # æ™ºèƒ½å»¶è¿Ÿ
                    if i < len(filtered_urls):
                        delay = self.get_adaptive_delay()
                        time.sleep(delay)

                except Exception as e:
                    print(f"âŒ å¼‚å¸¸: {e}")
                    results.append({'url': futures[future], 'success': False, 'error': str(e)})

        # 5. æ›´æ–°å…ƒæ•°æ®
        self._update_metadata(target_date, results)

        # 6. ç”ŸæˆæŠ¥å‘Š
        report = self._generate_report(results, target_date)

        print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘       çˆ¬å–å®ŒæˆæŠ¥å‘Š           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ æ€»æ•°: {report['total']:3d}                   â•‘
â•‘ æˆåŠŸ: {report['success']:3d}  ({report['success_rate']:5s})          â•‘
â•‘ å¤±è´¥: {report['failed']:3d}                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """)

        return report


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse

    parser = argparse.ArgumentParser(description='å¾®ä¿¡å…¬ä¼—å·æ‰¹é‡çˆ¬è™«')
    parser.add_argument('--date', default='today', help='ç›®æ ‡æ—¥æœŸ: today/yesterday/YYYY-MM-DD')
    parser.add_argument('--list-only', action='store_true', help='ä»…åˆ—å‡ºæ–‡ç« ,ä¸çˆ¬å–')
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„')

    args = parser.parse_args()

    try:
        scraper = WeChatBatchScraper(config_path=args.config)
        report = scraper.run(args.date, args.list_only)
        return 0 if report.get('error') is None else 1
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())
